{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kempess/emotion_recognition/blob/master/Face_Emotion_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Nama              : Muhammad Rizky Maulana Putra Suhaemi\n",
        "#NIM               : 1207050082\n",
        "#Mata Kuliah/Kelas : Praktikum Sistem Multimedia/D"
      ],
      "metadata": {
        "id": "A4VPRGjzmVw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kempess/emotion_recognition.git\n",
        "%cd emotion_recognition"
      ],
      "metadata": {
        "id": "a-pLVTdqyYTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install -U kora\n",
        "!pip install deepface\n",
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "TxGPq4uuPzsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tujuan dari perintah-perintah !pip install adalah untuk menginstal library TensorFlow, Keras, dan kora ke dalam lingkungan Python yang digunakan.\n",
        "\n",
        "1. !pip install tensorflow digunakan untuk menginstal library TensorFlow. TensorFlow adalah framework yang populer digunakan untuk membangun dan melatih model jaringan saraf tiruan.\n",
        "\n",
        "2. !pip install keras digunakan untuk menginstal library Keras. Keras adalah sebuah high-level neural networks API yang berjalan di atas TensorFlow. Keras menyediakan antarmuka yang lebih sederhana untuk membangun dan melatih model jaringan saraf tiruan.\n",
        "\n",
        "3. !pip install -U kora digunakan untuk menginstal library kora. Kora adalah library Python yang menyediakan akses ke berbagai alat dan fungsi yang berguna dalam lingkungan Google Colab, seperti mengunggah file ke Google Drive."
      ],
      "metadata": {
        "id": "1gCKJC5YmxzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntax ini bertujuan untuk membangun sebuah model pengenalan emosi pada gambar wajah yang menggunakan Deep Learning. Dengan menggunakan model tersebut, kita dapat mendeteksi dan memprediksi emosi yang terkandung dalam sebuah gambar wajah yang diimplementasikan dengan TensorFlow."
      ],
      "metadata": {
        "id": "aqtDpMmuitD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import subprocess\n",
        "from kora.drive import upload_public\n",
        "from google.colab.patches import cv2_imshow\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "from deepface import DeepFace\n",
        "import IPython.display as ipd\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "mode = \"display\"\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "def emotion_recog(frame):\n",
        "    model.load_weights('model.h5')\n",
        "\n",
        "    # Prevents OpenCL usage and unnecessary logging messages\n",
        "    cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "    # Dictionary which assigns each label an emotion (alphabetical order)\n",
        "    emotion_dict = {\n",
        "        0: \"Angry\",\n",
        "        1: \"Disgusted\",\n",
        "        2: \"Fearful\",\n",
        "        3: \"Happy\",\n",
        "        4: \"Neutral\",\n",
        "        5: \"Sad\",\n",
        "        6: \"Surprised\"\n",
        "    }\n",
        "\n",
        "    # frame = cv2.imread(\"image1.jpg\")\n",
        "    facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = facecasc.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 255), 3)\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
        "        prediction = model.predict(cropped_img)\n",
        "        maxindex = int(np.argmax(prediction))\n",
        "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "    # cv2_imshow(frame)\n",
        "    return frame"
      ],
      "metadata": {
        "id": "Vh17ckzZMm8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan mengenai library tensorflow :\n",
        "1. import tensorflow as tf: Mengimpor library TensorFlow yang digunakan untuk membangun dan melatih artificial neural network models.\n",
        "\n",
        "2. from tensorflow.keras.models import Sequential: Mengimpor class Sequential dari library tensorflow.keras.models yang digunakan untuk membangun artificial neural network models dengan urutan linear dari layer-layer yang didefinisikan.\n",
        "\n",
        "3. from tensorflow.keras.layers import Dense, Dropout, Flatten: Mengimpor class Dense, Dropout, dan Flatten dari library tensorflow.keras.layers yang digunakan untuk mendefinisikan jenis-jenis layer dalam artificial neural network models.\n",
        "\n",
        "4. from tensorflow.keras.layers import Conv2D: Mengimpor class Conv2D dari library tensorflow.keras.layers yang digunakan untuk mendefinisikan layer konvolusi dalam artificial neural network models.\n",
        "\n",
        "5. from tensorflow.keras.optimizers import Adam: Mengimpor class Adam dari library tensorflow.keras.optimizers yang digunakan untuk mendefinisikan algoritma optimizer Adam yang digunakan saat melatih model.\n",
        "\n",
        "6. from tensorflow.keras.layers import MaxPooling2D: Mengimpor class MaxPooling2D dari library tensorflow.keras.layers yang digunakan untuk mendefinisikan layer max pooling dalam artificial neural network models.\n",
        "\n",
        "7. from tensorflow.keras.preprocessing.image import ImageDataGenerator: Mengimpor class ImageDataGenerator dari library tensorflow.keras.preprocessing.image yang digunakan untuk melakukan augmentasi data gambar saat melatih model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Exg3vailkIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = 'https://gitlab.com/kempess/praktikum-sismul/-/raw/master/Asset-Face-Detection/facedetection.mp4'\n",
        "subprocess.run(['ffmpeg',\n",
        "                '-i',\n",
        "                input_file,\n",
        "                '-qscale',\n",
        "                '0',\n",
        "                'facedetection.mp4',\n",
        "                '-loglevel',\n",
        "                'quiet']\n",
        "              )"
      ],
      "metadata": {
        "id": "MMNCpybUYagZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "source code tersebut menggunakan subprocess untuk menjalankan perintah ffmpeg yang akan mengolah file video. lalu kita tampilkan video dalam format HTML di dalam notebook ini"
      ],
      "metadata": {
        "id": "qdpR4Zxni_kO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "url = 'https://gitlab.com/kempess/praktikum-sismul/-/raw/master/Asset-Face-Detection/facedetection.mp4'\n",
        "\n",
        "HTML(f\"\"\"<video src=\"{url}\" width=500 controls/>\"\"\")\n"
      ],
      "metadata": {
        "id": "NZF3mqEOaOS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita demonstrasikan menggunakan resource gambar, apakah pendeteksi sudah aktif atau belum dengan beberapa sample"
      ],
      "metadata": {
        "id": "wsK-YknmjQ7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = cv2.imread(\"image1.jpg\")\n",
        "output = emotion_recog(input)\n",
        "cv2_imshow(output)"
      ],
      "metadata": {
        "id": "izWK5pnkTGfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = cv2.imread(\"image3.jpg\")\n",
        "output = emotion_recog(input)\n",
        "cv2_imshow(output)"
      ],
      "metadata": {
        "id": "PlePY4PUUMkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = cv2.imread(\"image2.jpg\")\n",
        "output = emotion_recog(input)\n",
        "cv2_imshow(output)"
      ],
      "metadata": {
        "id": "0eWd4K7PTgTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = cv2.imread(\"image5.jpg\")\n",
        "output = emotion_recog(input)\n",
        "cv2_imshow(output)"
      ],
      "metadata": {
        "id": "zh0zrFsrTsNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = cv2.imread(\"image6.jpg\")\n",
        "output = emotion_recog(input)\n",
        "cv2_imshow(output)"
      ],
      "metadata": {
        "id": "HxxWgBqJT3i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah itu kita akan membaca sebuah file video, memproses setiap frame video menggunakan fungsi emotion_recog, dan juga menyimpan video hasil pengolahan ke dalam file output.avi"
      ],
      "metadata": {
        "id": "9aVjHsYljqt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture('https://gitlab.com/kempess/praktikum-sismul/-/raw/master/Asset-Face-Detection/facedetection.mp4')\n",
        "ret, frame = cap.read()\n",
        "\n",
        "frame_height, frame_width, _ = frame.shape  # Unpack all three values\n",
        "\n",
        "out = cv2.VideoWriter('output.avi', cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 10, (frame_width, frame_height))\n",
        "print(\"Processing Video...\")\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        out.release()\n",
        "        break\n",
        "    output = emotion_recog(frame)\n",
        "    out.write(output)\n",
        "\n",
        "out.release()\n",
        "print(\"Done processing video\")"
      ],
      "metadata": {
        "id": "6EntKIkbUpeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KOnversikan file video output.avi menjadi file video baru output.mp4 dengan menggunakan perintah ffmpeg melalui subprocess.run"
      ],
      "metadata": {
        "id": "mz-rmJDejzv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "input_file = '/content/emotion_recognition/output.avi'\n",
        "output_file = 'output.mp4'\n",
        "\n",
        "subprocess.run(['ffmpeg',\n",
        "                '-i',\n",
        "                input_file,\n",
        "                '-qscale',\n",
        "                '0',\n",
        "                'output.mp4',\n",
        "                '-loglevel',\n",
        "                'quiet'])\n"
      ],
      "metadata": {
        "id": "RGdL0vB3bOei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "kita jalankan perintah ls pada sistem operasi untuk menampilkan daftar file dan direktori dalam format yang terformat dengan informasi tambahan seperti ukuran, tanggal pembuatan, dan hak akses, serta dengan penyorotan warna"
      ],
      "metadata": {
        "id": "HWvgGD34kHsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -GFlash --color"
      ],
      "metadata": {
        "id": "TP04i2_LbZSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tampilkan video hasil dari pemrosesan dari awal yaitu output.mp4"
      ],
      "metadata": {
        "id": "yYmjcevZkVlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = upload_public('output.mp4')\n",
        "# then display it\n",
        "from IPython.display import HTML\n",
        "HTML(f\"\"\"<video src={url} width=500 controls/>\"\"\")"
      ],
      "metadata": {
        "id": "w6F6j9jiba88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ],
      "metadata": {
        "id": "B7dcATRwSnDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "CCuniRC5SX5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image1/jpg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "J6IWHsktFf5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import base64\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the pre-trained emotion detection model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "model.load_weights('model.h5')\n",
        "\n",
        "# Load the face cascade for face detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Function to convert JS response to OpenCV Image\n",
        "def js_to_image(js_reply):\n",
        "    # decode base64 image\n",
        "    image_bytes = js_reply.encode('utf-8')\n",
        "    image_array = np.frombuffer(base64.b64decode(image_bytes), dtype=np.uint8)\n",
        "    # decode image\n",
        "    img = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
        "    return img\n",
        "\n",
        "# Function to convert overlay of bbox into bytes\n",
        "def bbox_to_bytes(bbox_array):\n",
        "    # encode image as jpeg\n",
        "    _, buffer = cv2.imencode('.jpeg', bbox_array)\n",
        "    # convert to bytes\n",
        "    bbox_bytes = base64.b64encode(buffer)\n",
        "    return bbox_bytes\n",
        "\n",
        "# Start streaming video from webcam\n",
        "video_stream = cv2.VideoCapture(0)\n",
        "\n",
        "# Dictionary mapping emotion labels to their names\n",
        "emotion_dict = {\n",
        "    0: \"Angry\",\n",
        "    1: \"Disgusted\",\n",
        "    2: \"Fearful\",\n",
        "    3: \"Happy\",\n",
        "    4: \"Neutral\",\n",
        "    5: \"Sad\"\n",
        "}\n",
        "\n",
        "# Label for video\n",
        "label_html = 'Capturing...'\n",
        "\n",
        "# Initialize bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "\n",
        "while True:\n",
        "    ret, img = video_stream.read()\n",
        "\n",
        "    if not ret:\n",
        "        continue\n",
        "    # Create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480, 640, 4], dtype=np.uint8)\n",
        "\n",
        "    # Grayscale image for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Get face region coordinates\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    # Get face bounding box for overlay\n",
        "    for (x, y, w, h) in faces:\n",
        "        bbox_array = cv2.rectangle(bbox_array, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "    bbox_array[:, :, 3] = (bbox_array.max(axis=2) > 0).astype(int) * 255\n",
        "\n",
        "    # Convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "\n",
        "    # Update bbox so the next frame gets a new overlay\n",
        "    bbox = bbox_bytes"
      ],
      "metadata": {
        "id": "qq4HKm4Wte_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import base64\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Load the pre-trained emotion detection model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "model.load_weights('model.h5')\n",
        "\n",
        "# Load the face cascade for face detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Function to convert JS response to OpenCV Image\n",
        "def js_to_image(js_reply):\n",
        "    # decode base64 image\n",
        "    image_bytes = js_reply.encode('utf-8')\n",
        "    image_array = np.frombuffer(base64.b64decode(image_bytes), dtype=np.uint8)\n",
        "    # decode image\n",
        "    img = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
        "    return img\n",
        "\n",
        "# Function to convert overlay of bbox into bytes\n",
        "def bbox_to_bytes(bbox_array):\n",
        "    # encode image as jpeg\n",
        "    _, buffer = cv2.imencode('.jpeg', bbox_array)\n",
        "    # convert to bytes\n",
        "    bbox_bytes = base64.b64encode(buffer)\n",
        "    return bbox_bytes\n",
        "\n",
        "# Start streaming video from webcam\n",
        "video_stream = cv2.VideoCapture(0)\n",
        "\n",
        "# Dictionary mapping emotion labels to their names\n",
        "emotion_dict = {\n",
        "    0: \"Angry\",\n",
        "    1: \"Disgusted\",\n",
        "    2: \"Fearful\",\n",
        "    3: \"Happy\",\n",
        "    4: \"Neutral\",\n",
        "    5: \"Sad\"\n",
        "}\n",
        "\n",
        "# Label for video\n",
        "label_html = 'Capturing...'\n",
        "\n",
        "# Initialize bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "\n",
        "# Display video stream using IPython\n",
        "display(HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" autoplay>\n",
        "    <img id=\"img\" src=\"\" width=\"640\" height=\"480\" />\n",
        "</video>\n",
        "\"\"\"))\n",
        "\n",
        "while True:\n",
        "    ret, img = video_stream.read()\n",
        "\n",
        "    if not ret:\n",
        "        continue\n",
        "\n",
        "    # Create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480, 640, 4], dtype=np.uint8)\n",
        "\n",
        "    # Grayscale image for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Get face region coordinates\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    # Get face bounding box for overlay\n",
        "    for (x, y, w, h) in faces:\n",
        "        bbox_array = cv2.rectangle(bbox_array, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "    bbox_array[:, :, 3] = (bbox_array.max(axis=2) > 0).astype(int) * 255\n",
        "\n",
        "    # Convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "\n",
        "    # Update bbox so the next frame gets a new overlay\n",
        "    bbox = bbox_bytes\n",
        "\n",
        "    # Display the image in the notebook\n",
        "    display(HTML(f'<img src=\"data:image/jpeg;base64,{bbox}\" width=\"640\" height=\"480\" />'))\n"
      ],
      "metadata": {
        "id": "8wOqD13Z8h28"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}